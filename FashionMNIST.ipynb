{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Set the seed for random operations. \n",
    "# This let our experiments to be reproducible. \n",
    "tf.random.set_seed(1234)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Fashion MNIST - Multi-class classification\n",
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load built-in dataset\n",
    "# ---------------------\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split in training and validation sets\n",
    "# e.g., 50000 samples for training and 10000 samples for validation\n",
    "\n",
    "x_valid = x_train[50000:, ...]  #### Modified to have a faster evaluation\n",
    "y_valid = y_train[50000:, ...]  #### Modified to have a faster evaluation\n",
    "\n",
    "x_train = x_train[:50000, ...]\n",
    "y_train = y_train[:50000, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create Training Dataset object\n",
    "# ------------------------------\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "\n",
    "# Shuffle\n",
    "train_dataset = train_dataset.shuffle(buffer_size=x_train.shape[0])\n",
    "\n",
    "# Normalize images\n",
    "def normalize_img(x_, y_):\n",
    "    return tf.cast(x_, tf.float32) / 255., y_\n",
    "\n",
    "train_dataset = train_dataset.map(normalize_img)\n",
    "\n",
    "# 1-hot encoding <- for categorical cross entropy\n",
    "def to_categorical(x_, y_):\n",
    "    return x_, tf.one_hot(y_, depth=10)\n",
    "\n",
    "train_dataset = train_dataset.map(to_categorical)\n",
    "\n",
    "# Divide in batches\n",
    "bs = 32\n",
    "train_dataset = train_dataset.batch(bs)\n",
    "\n",
    "# Repeat\n",
    "# Without calling the repeat function the dataset \n",
    "# will be empty after consuming all the images\n",
    "train_dataset = train_dataset.repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Validation Dataset  \n",
    "# -----------------------\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((x_valid, y_valid))\n",
    "\n",
    "# Normalize images\n",
    "valid_dataset = valid_dataset.map(normalize_img)\n",
    "\n",
    "# 1-hot encoding\n",
    "valid_dataset = valid_dataset.map(to_categorical)\n",
    "\n",
    "# Divide in batches\n",
    "valid_dataset = valid_dataset.batch(bs)\n",
    "\n",
    "# Repeat\n",
    "valid_dataset = valid_dataset.repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Test Dataset\n",
    "# -------------------\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "\n",
    "test_dataset = test_dataset.map(normalize_img)\n",
    "\n",
    "test_dataset = test_dataset.map(to_categorical)\n",
    "\n",
    "test_dataset = test_dataset.batch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fashion MNIST classification\n",
    "# ----------------------------\n",
    "\n",
    "# x: 28x28\n",
    "# y: 10 classes\n",
    "\n",
    "# Create Model\n",
    "# ------------\n",
    "which_model = 'base'\n",
    "\n",
    "if which_model == 'base':\n",
    "    # Create base model (e.g., Input -> Hidden -> Out)\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Flatten(input_shape=(28, 28))) # or as a list\n",
    "    model.add(tf.keras.layers.Dense(units=1000, activation=tf.keras.activations.sigmoid))\n",
    "    model.add(tf.keras.layers.Dense(units=10, activation=tf.keras.activations.softmax))\n",
    "elif which_model == 'base_dropout':\n",
    "    # Create model with Dropout layer\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Flatten(input_shape=(28, 28))) # or as a list\n",
    "    model.add(tf.keras.layers.Dense(units=1000, activation=tf.keras.activations.sigmoid))\n",
    "    model.add(tf.keras.layers.Dropout(0.3))\n",
    "    model.add(tf.keras.layers.Dense(units=10, activation=tf.keras.activations.softmax))\n",
    "elif which_model == 'base_weight_decay':\n",
    "    # Create model with weights penalty (L2 regularization)\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Flatten(input_shape=(28, 28))) # or as a list\n",
    "    model.add(tf.keras.layers.Dense(units=1000, \n",
    "                                    activation=tf.keras.activations.sigmoid,\n",
    "                                    kernel_regularizer=tf.keras.regularizers.l2(0.0001)))\n",
    "    model.add(tf.keras.layers.Dense(units=10, \n",
    "                                    activation=tf.keras.activations.softmax,\n",
    "                                    kernel_regularizer=tf.keras.regularizers.l2(0.0001)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1000)              785000    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                10010     \n",
      "=================================================================\n",
      "Total params: 795,010\n",
      "Trainable params: 795,010\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'dense/kernel:0' shape=(784, 1000) dtype=float32, numpy=\n",
       " array([[-0.02025537, -0.04190592,  0.01717176, ...,  0.04665055,\n",
       "         -0.00734134,  0.02484443],\n",
       "        [ 0.02537891,  0.03631793, -0.05004032, ...,  0.03651362,\n",
       "          0.04028282,  0.01237207],\n",
       "        [-0.02591686,  0.05758444,  0.00186032, ..., -0.0287274 ,\n",
       "          0.01839119, -0.04242979],\n",
       "        ...,\n",
       "        [-0.03051765,  0.00858321,  0.02899709, ...,  0.02249118,\n",
       "         -0.00020791,  0.01880491],\n",
       "        [-0.03774619,  0.04266834, -0.01642644, ...,  0.03594344,\n",
       "         -0.02973929,  0.05186799],\n",
       "        [ 0.03166064,  0.00914184, -0.02561587, ...,  0.05204986,\n",
       "          0.01391875,  0.02314176]], dtype=float32)>,\n",
       " <tf.Variable 'dense/bias:0' shape=(1000,) dtype=float32, numpy=\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'dense_1/kernel:0' shape=(1000, 10) dtype=float32, numpy=\n",
       " array([[ 0.0150312 , -0.03034641,  0.07671789, ..., -0.07305414,\n",
       "          0.07354616, -0.02986033],\n",
       "        [ 0.03576598,  0.01574902, -0.01380333, ..., -0.04729154,\n",
       "          0.03474263, -0.02929928],\n",
       "        [-0.0418331 , -0.04596222,  0.06472538, ..., -0.022781  ,\n",
       "          0.0591047 , -0.07246497],\n",
       "        ...,\n",
       "        [ 0.04544161, -0.0198839 , -0.04404456, ..., -0.02130157,\n",
       "          0.03139233, -0.07465447],\n",
       "        [ 0.06362329, -0.05678294,  0.01412826, ...,  0.04052677,\n",
       "          0.06368086, -0.02198678],\n",
       "        [ 0.02892062,  0.04762707,  0.04728238, ..., -0.04590759,\n",
       "         -0.02109669, -0.07623676]], dtype=float32)>,\n",
       " <tf.Variable 'dense_1/bias:0' shape=(10,) dtype=float32, numpy=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize created model as a table\n",
    "model.summary()\n",
    "\n",
    "# Visualize initialized weights\n",
    "model.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the model for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization params\n",
    "# -------------------\n",
    "\n",
    "# Loss\n",
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "# learning rate\n",
    "lr = 1e-3\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "# -------------------\n",
    "\n",
    "# Validation metrics\n",
    "# ------------------\n",
    "\n",
    "metrics = ['accuracy']\n",
    "# ------------------\n",
    "\n",
    "# Compile Model\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 1563 steps, validate for 313 steps\n",
      "Epoch 1/2\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 0.5149 - accuracy: 0.8142 - val_loss: 0.4108 - val_accuracy: 0.8523\n",
      "Epoch 2/2\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 0.3944 - accuracy: 0.8559 - val_loss: 0.3832 - val_accuracy: 0.8586\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x159749650>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "cwd = os.getcwd()\n",
    "\n",
    "exps_dir = os.path.join(cwd, 'overfitting_experiments')\n",
    "if not os.path.exists(exps_dir):\n",
    "    os.makedirs(exps_dir)\n",
    "\n",
    "now = datetime.now().strftime('%b%d_%H-%M-%S')\n",
    "\n",
    "exp_name = which_model\n",
    "\n",
    "exp_dir = os.path.join(exps_dir, exp_name + '_' + str(now))\n",
    "if not os.path.exists(exp_dir):\n",
    "    os.makedirs(exp_dir)\n",
    "    \n",
    "callbacks = []\n",
    "\n",
    "# Model checkpoint\n",
    "# ----------------\n",
    "ckpt_dir = os.path.join(exp_dir, 'ckpts')\n",
    "if not os.path.exists(ckpt_dir):\n",
    "    os.makedirs(ckpt_dir)\n",
    "\n",
    "ckpt_callback = tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(ckpt_dir, 'cp_{epoch:02d}.ckpt'), \n",
    "                                                   save_weights_only=True)  # False to save the model directly\n",
    "callbacks.append(ckpt_callback)\n",
    "\n",
    "# ----------------\n",
    "\n",
    "# Visualize Learning on Tensorboard\n",
    "# ---------------------------------\n",
    "tb_dir = os.path.join(exp_dir, 'tb_logs')\n",
    "if not os.path.exists(tb_dir):\n",
    "    os.makedirs(tb_dir)\n",
    "    \n",
    "# By default shows losses and metrics for both training and validation\n",
    "tb_callback = tf.keras.callbacks.TensorBoard(log_dir=tb_dir,\n",
    "                                             profile_batch=0,\n",
    "                                             histogram_freq=1)  # if 1 shows weights histograms\n",
    "callbacks.append(tb_callback)\n",
    "\n",
    "# Early Stopping\n",
    "# --------------\n",
    "early_stop = False\n",
    "if early_stop:\n",
    "    es_callback = tf.keras.callback.EarlyStopping(monitor='val_loss', patience=10)\n",
    "    callbacks.append(es_callback)\n",
    "\n",
    "# ---------------------------------\n",
    "\n",
    "model.fit(x=train_dataset,\n",
    "          epochs=2,  #### set repeat in training dataset\n",
    "          steps_per_epoch=int(np.ceil(x_train.shape[0] / bs)),\n",
    "          validation_data=valid_dataset,\n",
    "          validation_steps=int(np.ceil(x_valid.shape[0] / bs)), \n",
    "          callbacks=callbacks)\n",
    "\n",
    "# How to visualize Tensorboard\n",
    "\n",
    "# 1. tensorboard --logdir EXPERIMENTS_DIR --port PORT     <- from terminal\n",
    "# 2. localhost:PORT   <- in your browser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4091281944036484, 0.8531]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try a different way to give data to model \n",
    "# using directly the NumPy arrays\n",
    "\n",
    "# model.load_weights('/path/to/checkpoint')  # use this if you want to restore saved model\n",
    "\n",
    "eval_out = model.evaluate(x=x_test / 255.,\n",
    "                          y=tf.keras.utils.to_categorical(y_test),\n",
    "                          verbose=0)\n",
    "\n",
    "eval_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Image'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-87e7e965441f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Compute output given x\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mshoe_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'shoe.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'L'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mshoe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshoe_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'Image'"
     ]
    }
   ],
   "source": [
    "# Compute output given x\n",
    "import Image\n",
    "shoe_img = Image.open('shoe.png').convert('L')\n",
    "\n",
    "shoe_arr = np.expand_dims(np.array(shoe_img), 0)\n",
    "\n",
    "out_softmax = model.predict(x=shoe_arr / 255.)\n",
    "\n",
    "out_softmax  # is already a probability distribution (softmax)\n",
    "\n",
    "# Get predicted class as the index corresponding to the maximum value in the vector probability\n",
    "predicted_class = tf.argmax(out_softmax, 1)\n",
    "predicted_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
